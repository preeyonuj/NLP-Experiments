{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "675dc0d4-680e-4dd1-9b9d-b31e1a8fdc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0a248b9-8eb6-44b5-8144-92a9cd11f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name and extracting data from it\n",
    "file_name = r\"movie_lines.txt\"\n",
    "lines = []\n",
    "with open(file_name) as f:\n",
    "    doc = f.readlines()\n",
    "\n",
    "for line in doc:\n",
    "    lines.append(line.split(\" +++$+++ \")[-1].replace(\"\\n\",\"\"))\n",
    "\n",
    "# lines = \" \".join(lines)\n",
    "    \n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5faddd97-27d7-4629-a71f-c2bdc42e7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "min_words = 5\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "595c5b89-34f3-44a0-81b6-0e9b04d2412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "lines = [re.sub('[^a-zA-Z0-9 ]', '', line) for line in lines]\n",
    "\n",
    "# Consider only those sentences with more than 2 words\n",
    "lines = [text for text in lines if text.count(' ') >= min_words]\n",
    "\n",
    "# Consider a subset\n",
    "lines = lines[:1000]\n",
    "\n",
    "# Tokenizing the words \n",
    "vectorize = Tokenizer()\n",
    "vectorize.fit_on_texts(lines)\n",
    "vectorized_lines = vectorize.texts_to_sequences(lines)\n",
    "\n",
    "# Dataset based stats\n",
    "total_vocab = sum(len(s) for s in vectorized_lines)\n",
    "word_count = len(vectorize.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0cb7cb73-f4fe-477e-be8c-50fafa171e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(data, window_size, total_vocab):\n",
    "    # Total length of the window considering the target word\n",
    "    total_length = window_size*2\n",
    "    \n",
    "    # Individually considering each sentence\n",
    "    for text in data:\n",
    "        text_len = len(text)\n",
    "        for idx, word in enumerate(text):\n",
    "            \n",
    "            context_word = []\n",
    "            target   = []            \n",
    "            begin = idx - window_size\n",
    "            end = idx + window_size + 1\n",
    "            \n",
    "            # Consider surrounding words of the target words. Special condition to avoid negative indexing\n",
    "            context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n",
    "            target.append(word)\n",
    "            \n",
    "            # Padding sequences in case of starting words and ending words of sentences\n",
    "            contextual = sequence.pad_sequences(context_word, maxlen=total_length)\n",
    "            final_target = np_utils.to_categorical(target, total_vocab)\n",
    "            yield(contextual, final_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3724a73d-d8d2-422b-80f5-f4ba4f60689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 96778.89892441034\n",
      "1 81283.45058482885\n",
      "2 74931.77163121104\n",
      "3 68754.67902367562\n",
      "4 62681.44745000824\n",
      "5 56809.88444168866\n",
      "6 51211.7634006273\n",
      "7 46085.82725789177\n",
      "8 41575.64151547686\n",
      "9 37674.032532867684\n"
     ]
    }
   ],
   "source": [
    "# Simple Model with Embedding Layer with Categorical Crossentropy loss and Adam Optimizer\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n",
    "model.add(Dense(total_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "for i in range(10):\n",
    "    cost = 0\n",
    "    for contextual, final_target in cbow_model(vectorized_lines, window_size, total_vocab):\n",
    "        cost += model.train_on_batch(contextual, final_target)\n",
    "    print(i, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24511f22-efa5-4605-a816-ff1a67bccb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must match dimension of model output\n",
    "dimensions = 100\n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Storing Word Vectors with weights onto disk\n",
    "with open('vectors.txt' ,'w') as vect_file:\n",
    "    vect_file.write('{} {}\\n'.format(len(vectorize.word_index.items()), dimensions))\n",
    "    for text, i in vectorize.word_index.items():\n",
    "        \n",
    "        # Special condition for last word embeddeding to avoid adding extra empty line\n",
    "        if i == len(vectorize.word_index):\n",
    "            final_vec = ' '.join(map(str, list(weights[i, :])))\n",
    "            vect_file.write('{} {}'.format(text, final_vec))\n",
    "            continue\n",
    "            \n",
    "        final_vec = ' '.join(map(str, list(weights[i, :])))\n",
    "        vect_file.write('{} {}\\n'.format(text, final_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "74e4aa91-1354-4cc5-b87b-0a6747063eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gay', 0.3722064197063446),\n",
       " ('gonna', 0.32852739095687866),\n",
       " ('cop', 0.31805941462516785),\n",
       " ('introduction', 0.315818190574646),\n",
       " ('plan', 0.3115581274032593),\n",
       " ('heres', 0.3040046691894531),\n",
       " ('sitting', 0.29972460865974426),\n",
       " ('ending', 0.29890140891075134),\n",
       " ('spare', 0.2936362028121948),\n",
       " ('body', 0.291808158159256)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Word Vector into Gensim Word2Vec model\n",
    "cbow_output = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)\n",
    "\n",
    "# Random word checked\n",
    "cbow_output.most_similar(positive=['good'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
